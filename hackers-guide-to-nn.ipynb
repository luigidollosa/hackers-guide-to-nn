{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker's Guide to Neural Networks\n",
    "* Python code w/ personal notes and experiments from [Andrej Karpathy's tutorial](http://karpathy.github.io/neuralnets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Real-valued Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit with Single Gate\n",
    "$f\\left(x,y\\right)\\ =\\ xy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forwardMultiplyGate(x,y):\n",
    "    return x * y\n",
    "\n",
    "forwardMultiplyGate(-2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 1: Random Local Search\n",
    "* throw numbers at the wall and see what sticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = -2, 3\n",
    "best_x, best_y = x, y\n",
    "best_out = -float(\"inf\")\n",
    "tweak_amount = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.990146983617037, 3.009853016382963, -6.008535150184225)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random as r\n",
    "for i in range(100):\n",
    "    random_ = (r.random() * 2 - 1)\n",
    "    x_try = x + tweak_amount * random_\n",
    "    y_try = y + tweak_amount * random_\n",
    "    out = forwardMultiplyGate(x_try, y_try)\n",
    "    \n",
    "    if (out > best_out):\n",
    "        best_out, best_x, best_y = out, x_try, y_try\n",
    "\n",
    "best_x, best_y, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 2: Numerical Gradient\n",
    "* finding the derivatives by tweaking the knobs for each pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.00000000000189, -2.0000000000042206)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = -2, 3\n",
    "out = forwardMultiplyGate(x, y)\n",
    "h = 0.0001\n",
    "\n",
    "out_x = forwardMultiplyGate(x + h, y)\n",
    "x_derivative = float((out_x - out) / h)\n",
    "out_y = forwardMultiplyGate(x, y + h)\n",
    "y_derivative = float((out_y - out) / h)\n",
    "\n",
    "x_derivative, y_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{df\\left(x,y\\right)}{dx}=\\frac{f\\left(x+h,\\ y\\right)\\ -\\ f\\left(x,y\\right)}{h}$\n",
    "\n",
    "Think of the derivative as a check and balance. If it is (+), it tells the variable that if the function ought to increase, this is where the function will be going (combined with the magnitude). The derivatives, whether they evaluate to +/- (times magnitude), will be forcing/pulling the function to proceed to the direction where the function will increase (hence, the gradient). It is a value that indicates whether nudge (h) gives a variable a +/- slope when evaluated w/ the original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.87059999999986"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "x += step_size * x_derivative\n",
    "y += step_size * y_derivative\n",
    "out_new = forwardMultiplyGate(x, y)\n",
    "\n",
    "out_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step size is the key here. Turn it (-), the direction will turn the opposite way. When (+), it inclines the derivatives to _increase_ the function (following the gradient). It sort of amplifies, little by little, the force and direction given by the derivatives. It 'commands' the circuit to proceed with the derivatives whatever is their signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expt: Try to see with more iterations if the gradient is really towards increasing the function..   \n",
    "Result: It is! Except if step to (-)step, opposite direction of gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old x: -0.9999, x derivative: 1.0, new x: -0.99\n",
      "old y: -1.0001, y derivative: -1.0, new y: 0.99\n",
      "out: -0.9801\n",
      "\n",
      "old x: -0.980001, x derivative: 0.99, new x: -0.9801\n",
      "old y: -0.980199, y derivative: -0.99, new y: 0.9801\n",
      "out: -0.96059601\n",
      "\n",
      "old x: -0.960498, x derivative: 0.9801, new x: -0.970299\n",
      "old y: -0.96069402, y derivative: -0.9801, new y: 0.970299\n",
      "out: -0.941480149401\n",
      "\n",
      "old x: -0.941383119501, x derivative: 0.970299, new x: -0.96059601\n",
      "old y: -0.941577179301, y derivative: -0.970298999999, new y: 0.96059601\n",
      "out: -0.922744694428\n",
      "\n",
      "old x: -0.922648634827, x derivative: 0.96059601, new x: -0.9509900499\n",
      "old y: -0.922840754029, y derivative: -0.96059601, new y: 0.9509900499\n",
      "out: -0.904382075009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "x, y = -1, 1\n",
    "h = 0.0001\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    out = forwardMultiplyGate(x, y)\n",
    "    \n",
    "    out_x = forwardMultiplyGate(x + h, y)\n",
    "    x_derivative = (out_x - out) / h\n",
    "    \n",
    "    out_y = forwardMultiplyGate(x, y + h)\n",
    "    y_derivative = (out_y - out) / h\n",
    "    \n",
    "    x += step_size * x_derivative\n",
    "    y += step_size * y_derivative\n",
    "    out_new = forwardMultiplyGate(x, y)\n",
    "    \n",
    "    print \"old x: %s, x derivative: %s, new x: %s\\n\" % (out_x, x_derivative, x) + \\\n",
    "          \"old y: %s, y derivative: %s, new y: %s\\nout: %s\\n\" % (out_y, y_derivative, y, out_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 3: Analytical Gradient\n",
    "* for our function, it turns out that the derivatives of x, y are y, x respectively.\n",
    "\n",
    "$\\frac{df\\left(x,y\\right)}{dx}=\\frac{\\left(x+h\\right)y\\ -\\ xy}{h}=y$\n",
    "\n",
    "Instead of probing with h, compute the derivatives directly for each step because math.\n",
    "Btw, h presumes that whatever the sign of the derivative will be , it corresponds to the rate of growth. ie. increasing a little bit of a variable results to a rate of increase. If that rate is (+), this means growth; if that is (-), opposite of growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.8706"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = -2, 3 # re-initialize\n",
    "x_gradient, y_gradient = y, x # derived from separate evaluation\n",
    "\n",
    "x += step_size * x_gradient\n",
    "y += step_size * y_gradient\n",
    "out_new = forwardMultiplyGate(x, y)\n",
    "\n",
    "out_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuits with Multiple Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forwardMultiplyGate(a, b): return a * b\n",
    "def forwardAddGate(a, b): return a + b\n",
    "\n",
    "def forwardCircuit(x, y, z):\n",
    "    q = forwardAddGate(x, y)\n",
    "    f = forwardMultiplyGate(q, z) \n",
    "    return f\n",
    "\n",
    "x, y, z = -2, 5, -4\n",
    "forwardCircuit(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backpropagation\n",
    "* the chain rule, is really really useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n",
      "-11.5924\n"
     ]
    }
   ],
   "source": [
    "x, y, z = -2, 5, -4\n",
    "q = forwardAddGate(x, y)\n",
    "f = forwardMultiplyGate(q, z)\n",
    "print f\n",
    "\n",
    "derivative_f_wrt_z = q\n",
    "derivative_f_wrt_q = z\n",
    "\n",
    "derivative_q_wrt_x = 1\n",
    "derivative_q_wrt_y = 1\n",
    "\n",
    "derivative_f_wrt_x = derivative_f_wrt_q * derivative_q_wrt_x\n",
    "derivative_f_wrt_y = derivative_f_wrt_q * derivative_q_wrt_y\n",
    "\n",
    "step_size = 0.01\n",
    "x += step_size * derivative_f_wrt_x\n",
    "y += step_size * derivative_f_wrt_y\n",
    "z += step_size * derivative_f_wrt_z\n",
    "\n",
    "print forwardMultiplyGate(forwardAddGate(x, y), z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to think about this is that the circuit is a fitting-a-function exercise; and in our simple case, the gradient is just increasing the function w/c doesn't optimize anything yet. The chain rule, which is implemented via backpropagation, will signal each layer how much and where to go in terms of adjustments to satisfy the last function. It is a backward pass, where each layer influences the functions before it to adjust accordingly. \n",
    "\n",
    "The chain rule tells us formally what the sensitivity of the weights (and other variables) to the over-all change of the function. We can sort of trust that every layer and the layer before it communicates locally to produce a desired computation globally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with more iterations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.5924\n",
      "-11.191964\n",
      "-10.798638\n",
      "-10.412368\n",
      "-10.0331\n",
      "-9.66078\n",
      "-9.295354\n",
      "-8.936768\n",
      "-8.584968\n",
      "-8.2399\n",
      "-7.90151\n",
      "-7.569744\n",
      "-7.244548\n",
      "-6.925868\n",
      "-6.61365\n",
      "-6.30784\n",
      "-6.008384\n",
      "-5.715228\n",
      "-5.428318\n",
      "-5.1476\n"
     ]
    }
   ],
   "source": [
    "x, y, z = -2, 5, -4\n",
    "step_size = 0.01\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    dfdz = q\n",
    "    dfdq = z\n",
    "\n",
    "    dqdx = 1\n",
    "    dqdy = 1\n",
    "\n",
    "    dfdx = dfdq * dqdx\n",
    "    dfdy = dfdq * dqdy\n",
    "\n",
    "    x += step_size * dfdx\n",
    "    y += step_size * dfdy\n",
    "    z += step_size * dfdz\n",
    "    \n",
    "    print forwardMultiplyGate(forwardAddGate(x, y), z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expt: Now, experiment with the chain rule by adding a basic cost function at the end. (a bit of a fast forward)   \n",
    "Result: It works! Finds the proper inputs to minimize the function, instead of just ascending the function.   \n",
    "(c gradually decreases, so f gets closer to k). Beautiful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: -1.008\t y: 2.992\t z: 3.994\n",
      "f: 8.0\t c: 2.0\n",
      "\n",
      "x: -1.0157\t y: 2.9843\t z: 3.9882\n",
      "f: 7.9241\t c: 1.8511\n",
      "\n",
      "x: -1.0231\t y: 2.9769\t z: 3.9827\n",
      "f: 7.8513\t c: 1.7137\n",
      "\n",
      "x: -1.0302\t y: 2.9698\t z: 3.9773\n",
      "f: 7.7816\t c: 1.587\n",
      "\n",
      "x: -1.037\t y: 2.963\t z: 3.9722\n",
      "f: 7.7147\t c: 1.4701\n",
      "\n",
      "x: -1.0435\t y: 2.9565\t z: 3.9672\n",
      "f: 7.6506\t c: 1.3622\n",
      "\n",
      "x: -1.0498\t y: 2.9502\t z: 3.9625\n",
      "f: 7.589\t c: 1.2625\n",
      "\n",
      "x: -1.0559\t y: 2.9441\t z: 3.9579\n",
      "f: 7.5299\t c: 1.1703\n",
      "\n",
      "x: -1.0617\t y: 2.9383\t z: 3.9535\n",
      "f: 7.4732\t c: 1.0852\n",
      "\n",
      "x: -1.0673\t y: 2.9327\t z: 3.9492\n",
      "f: 7.4188\t c: 1.0064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y, z, k = -1, 3, 4, 6\n",
    "step_size = 0.001\n",
    "\n",
    "for i in range(10):    \n",
    "   \n",
    "    f = forwardMultiplyGate(forwardAddGate(x, y), z)\n",
    "    c = ((f - k) ** 2) / 2\n",
    "    \n",
    "    dfdz = q\n",
    "    dfdq = z\n",
    "\n",
    "    dqdx = 1\n",
    "    dqdy = 1\n",
    "\n",
    "    dfdx = dfdq * dqdx\n",
    "    dfdy = dfdq * dqdy\n",
    "    \n",
    "    # we want to follow the opposite of the gradient to minimize, not maximize, the cost\n",
    "    dcdf = k - f \n",
    "    dcdx = dcdf * dfdx\n",
    "    dcdy = dcdf * dfdy\n",
    "    dcdz = dcdf * dfdz\n",
    "\n",
    "    x += step_size * dcdx\n",
    "    y += step_size * dcdy\n",
    "    z += step_size * dcdz\n",
    "\n",
    "    print \"x: %s\\t y: %s\\t z: %s\\nf: %s\\t c: %s\\n\" \\\n",
    "          % (round(x, 4), round(y, 4), round(z, 4), round(f, 4), round(c, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Neuron\n",
    "* using sigmoid activation, so there is variation of values between 0 to 1, not just 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0\n",
      "-6\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "class Unit(object):\n",
    "    def __init__(self, value, grad):\n",
    "        self.value = value\n",
    "        self.grad = grad\n",
    "        \n",
    "class MultiplyGate(object):\n",
    "    def forward(self, u0, u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value * u1.value, 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        self.u0.grad += self.u1.value * self.utop.grad\n",
    "        self.u1.grad += self.u0.value * self.utop.grad\n",
    "        \n",
    "class AddGate(object):\n",
    "    def forward(self, u0, u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value + u1.value, 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        self.u0.grad += 1 * self.utop.grad\n",
    "        self.u1.grad += 1 * self.utop.grad\n",
    "    \n",
    "x = Unit(2,0)\n",
    "y = Unit(-3,0)\n",
    "print x.value, x.grad\n",
    "\n",
    "m = MultiplyGate()\n",
    "print m.forward(x,y).value\n",
    "\n",
    "a = AddGate()\n",
    "print a.forward(x, y).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid function    \n",
    "$sig\\left(x\\right)\\ =\\ \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "derivative   \n",
    "$\\frac{dsig\\left(x\\right)}{dx}=\\ sig\\left(x\\right)\\left(1-sig\\left(x\\right)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952574126822\n",
      "0.880797077978\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "class SigmoidGate(object):\n",
    "    def forward(self, u0):\n",
    "        self.u0 = u0\n",
    "        self.utop = Unit(sigmoid(self.u0.value), 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        s = sigmoid(self.u0.value)\n",
    "        self.u0.grad += (s * (1 - s)) * self.utop.grad\n",
    "\n",
    "print sigmoid(3)\n",
    "sg = SigmoidGate()\n",
    "print sg.forward(x).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Neuron (forward pass)__  \n",
    "* dot product of input & weights, + c (bias term), then feed to sigmoid\n",
    "* it yields a single value\n",
    "\n",
    "__Neuron (backward pass)__   \n",
    "* adjust global vars with computed gradients, the sequence is impt on the chain\n",
    "* started with gradient 1 from the end operation which is the sigmoid\n",
    "* it yields updates to every value (not just a single value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mulg0, self.mulg1 = MultiplyGate(), MultiplyGate()\n",
    "        self.addg0, self.addg1 = AddGate(), AddGate()\n",
    "        self.sg0 = SigmoidGate()\n",
    "\n",
    "    def forward(self, a, b, c, x, y):\n",
    "        ax = self.mulg0.forward(a, x)\n",
    "        by = self.mulg1.forward(b, y)\n",
    "        axby = self.addg0.forward(ax, by)\n",
    "        axbyc = self.addg1.forward(axby, c)\n",
    "        self.s = self.sg0.forward(axbyc)\n",
    "        return self.s\n",
    "    \n",
    "    def backward(self, a, b, c, x, y):\n",
    "        step_size = 0.01\n",
    "        self.s.grad = 1.0\n",
    "        \n",
    "        self.sg0.backward()\n",
    "        self.addg1.backward()\n",
    "        self.addg0.backward()\n",
    "        self.mulg1.backward()\n",
    "        self.mulg0.backward()\n",
    "        \n",
    "        a.value += step_size * a.grad\n",
    "        b.value += step_size * b.grad\n",
    "        c.value += step_size * c.grad\n",
    "        x.value += step_size * x.grad\n",
    "        y.value += step_size * y.grad\n",
    "        \n",
    "        return \"\\nvalues:\\na: %s, x: %s\\nb: %s, y: %s\\nc: %s\\n\" \\\n",
    "                % (a.value, x.value, b.value, y.value, c.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass 1:  0.880797077978\n",
      "\n",
      "backward pass 1:  \n",
      "values:\n",
      "a: 0.998950064146, x: -0.998950064146\n",
      "b: 2.00314980756, y: 3.00209987171\n",
      "c: -2.99895006415\n",
      "\n",
      "forward pass 2:  0.882550181622\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "a, b = Unit(1.0, 0.0), Unit(2.0, 0.0)\n",
    "x, y = Unit(-1.0, 0.0), Unit(3.0, 0.0)\n",
    "c = Unit(-3.0, 0.0)\n",
    "\n",
    "# process / output\n",
    "n = Neuron()\n",
    "print \"forward pass 1: \", n.forward(a, b, c, x, y).value\n",
    "print \"\\nbackward pass 1: \", n.backward(a, b, c, x, y)\n",
    "print \"forward pass 2: \", n.forward(a, b, c, x, y).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Gradients Check\n",
    "* just checking if the gradient is carried out correctly from above.\n",
    "* they're equal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.104997583592 0.314944774835 0.104989587345 0.104989587345 0.209971178827\n"
     ]
    }
   ],
   "source": [
    "a, b, c, x, y = 1, 2, -3, -1, 3\n",
    "h = 0.0001\n",
    "\n",
    "def forwardCircuitFast(a, b, c, x, y):\n",
    "    return 1 / (1 + math.exp(-(a*x + b*y + c)))\n",
    "\n",
    "a_grad = (forwardCircuitFast(a+h, b, c, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "b_grad = (forwardCircuitFast(a, b+h, c, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "c_grad = (forwardCircuitFast(a, b, c+h, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "x_grad = (forwardCircuitFast(a, b, c, x+h, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "y_grad = (forwardCircuitFast(a, b, c, x, y+h) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "\n",
    "print a_grad, b_grad, c_grad, x_grad, y_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "* linear classifier ala svm. ax + by + c, won't use activation\n",
    "* labels as 1, -1\n",
    "* stochastic gradient descent = pick a random pair, gradient descent on each one.\n",
    "* weights will define a linear boundary, ideal output:\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://cdn.pbrd.co/images/H4G0bnR.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.666666666667\n",
      "25 0.666666666667\n",
      "50 0.833333333333\n",
      "75 0.833333333333\n",
      "100 0.833333333333\n",
      "125 0.833333333333\n",
      "150 0.833333333333\n",
      "175 0.833333333333\n",
      "200 0.833333333333\n",
      "225 0.833333333333\n",
      "250 0.833333333333\n",
      "275 0.833333333333\n",
      "300 0.833333333333\n",
      "325 0.833333333333\n",
      "350 0.833333333333\n",
      "375 0.833333333333\n"
     ]
    }
   ],
   "source": [
    "# data, labels, weights, bias initalization\n",
    "data = [[1.2, 0.7], [-0.3, -0.5], [3.0, 0.1], [-0.1, -1.0], [-1.0, 1.1], [2.1, -3]]\n",
    "labels = [1, -1, 1, -1, -1, 1]\n",
    "a, b, c = 1, -2, -1\n",
    "\n",
    "def check(a, b, c):\n",
    "    num_cor = 0\n",
    "    for i in range(len(data)):\n",
    "        x, y, label = data[i][0], data[i][1], labels[i]\n",
    "        score = a*x + b*y + c\n",
    "        if ((score > 0.0 and label == 1) or (score < 0.0 and label == -1)): num_cor += 1\n",
    "    return num_cor / float(len(data))\n",
    "        \n",
    "for l in range(400):\n",
    "    i = int(r.random() * len(data))\n",
    "    x, y, label = data[i][0], data[i][1], labels[i]\n",
    "\n",
    "    if (l % 25 == 0): print l, check(a, b, c)\n",
    "    score = a*x + b*y + c\n",
    "    \n",
    "    # +/- assignment for the gradient of the function\n",
    "    pull = 0.0\n",
    "    if (label == 1 and score < 1.0): pull = 1.0\n",
    "    if (label == -1 and score > -1.0): pull = -1.0\n",
    "    \n",
    "    ss = 0.01\n",
    "    a += ss * (x * pull - a) # -a regularization\n",
    "    b += ss * (y * pull - b) # -b regularization\n",
    "    c += ss * (pull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm.. it gets stuck?\n",
    "\n",
    "Expt: Removing the 'regularization pull'..    \n",
    "Result: Seems like adding a regularization pull -a or -b (ie. x \\* pull - a) makes the circuit wiggle more (it can converge, but immediately steps out after), whereas removing it makes the circuit converge faster & more consistent after iterations. The reason is that dfda or dfdb is derived to be as df \\* dfdz \\* dzda (z=ax+by), which evaluates to the (+1 or -1) \\* 1 * (x or y). Maybe this happens because of the given data for this toy problem; perhaps in practice, regularization does prevent params from getting noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.666666666667 (1, -2, -1)\n",
      "25 0.666666666667 (1.0919999999999992, -1.9080000000000013, -1.02)\n",
      "50 0.666666666667 (1.1539999999999984, -1.815000000000002, -1.06)\n",
      "75 0.666666666667 (1.2429999999999974, -1.703000000000003, -1.09)\n",
      "100 0.666666666667 (1.324999999999997, -1.636000000000004, -1.07)\n",
      "125 0.666666666667 (1.402999999999996, -1.5210000000000048, -1.12)\n",
      "150 0.666666666667 (1.4599999999999953, -1.4280000000000053, -1.1500000000000001)\n",
      "175 0.666666666667 (1.5109999999999946, -1.357000000000006, -1.1900000000000002)\n",
      "200 0.833333333333 (1.5559999999999938, -1.2710000000000063, -1.2300000000000002)\n",
      "225 0.833333333333 (1.6299999999999937, -1.209000000000007, -1.1900000000000002)\n",
      "250 0.833333333333 (1.6459999999999932, -1.162000000000007, -1.2200000000000002)\n",
      "275 1.0 (1.744999999999993, -1.0760000000000078, -1.1700000000000002)\n",
      "300 1.0 (1.7989999999999924, -0.9880000000000081, -1.1900000000000002)\n",
      "325 1.0 (1.8719999999999923, -0.936000000000008, -1.1400000000000001)\n",
      "350 1.0 (1.910999999999992, -0.885000000000008, -1.1400000000000001)\n",
      "375 1.0 (1.9639999999999915, -0.8070000000000079, -1.1500000000000001)\n"
     ]
    }
   ],
   "source": [
    "# data, labels, weights, bias initalization\n",
    "data = [[1.2, 0.7], [-0.3, -0.5], [3.0, 0.1], [-0.1, -1.0], [-1.0, 1.1], [2.1, -3]]\n",
    "labels = [1, -1, 1, -1, -1, 1]\n",
    "a, b, c = 1, -2, -1\n",
    "\n",
    "def check(a, b, c):\n",
    "    num_cor = 0\n",
    "    for i in range(len(data)):\n",
    "        x, y, label = data[i][0], data[i][1], labels[i]\n",
    "        score = a*x + b*y + c\n",
    "        if ((score > 0.0 and label == 1) or (score < 0.0 and label == -1)): num_cor += 1\n",
    "    return num_cor / float(len(data))\n",
    "        \n",
    "for l in range(400):\n",
    "    i = int(r.random() * len(data))\n",
    "    x, y, label = data[i][0], data[i][1], labels[i]\n",
    "\n",
    "    if (l % 25 == 0): print l, check(a, b, c), (a, b, c)\n",
    "    score = a*x + b*y + c\n",
    "    \n",
    "    # +/- assignment for the gradient of the function\n",
    "    pull = 0.0\n",
    "    if (label == 1 and score < 1.0): pull = 1.0\n",
    "    if (label == -1 and score > -1.0): pull = -1.0\n",
    "    \n",
    "    # removed regularization pull for the meantime, these are the derived gradients\n",
    "    ss = 0.01\n",
    "    a += ss * (x * pull) \n",
    "    b += ss * (y * pull) \n",
    "    c += ss * (pull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generalizing the SVM into a Neural Network\n",
    "* 3 neurons, relu (take + values only) instead of sigmoid\n",
    "* the svm above was a single linear classifier; this one, each of the 3 neurons is a linear classifier + 1 classifier at the end\n",
    "* this is prototype code. in practical cases, refactor into cleaner data structures. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5\n",
      "25 0.5\n",
      "50 0.5\n",
      "75 0.666666666667\n",
      "100 0.666666666667\n",
      "125 0.666666666667\n",
      "150 0.666666666667\n",
      "175 0.833333333333\n",
      "200 1.0\n",
      "neuron 1: (0.0142470208072, 0.209841598511, 0.0688395305489)\n",
      "neuron 2: (0.846544886453, -0.0414664387715, 0.754752133392)\n",
      "neuron 3: (0.26275002475, -0.242594850296, 0.101098291734)\n",
      "final:    (0.0868653497678, 0.755706088511, -0.274861133175, -0.502431468581)\n",
      "\n",
      "225 1.0\n",
      "neuron 1: (0.0195696568318, 0.212946469525, 0.0732750605694)\n",
      "neuron 2: (0.891039365232, -0.0155113261501, 0.791830865708)\n",
      "neuron 3: (0.25208893556, -0.263314691372, 0.138299880896)\n",
      "final:    (0.0930295164937, 0.772024546842, -0.294772201055, -0.582431468581)\n",
      "\n",
      "250 1.0\n",
      "neuron 1: (0.0235264804562, 0.211567434377, 0.0759178841941)\n",
      "neuron 2: (0.925045021265, -0.0272360471051, 0.814566305247)\n",
      "neuron 3: (0.23683407559, -0.271208663951, 0.174041584377)\n",
      "final:    (0.0893862089507, 0.78217667273, -0.299561828091, -0.672431468581)\n",
      "\n",
      "275 1.0\n",
      "neuron 1: (0.0289892487662, 0.214754049225, 0.0804701911191)\n",
      "neuron 2: (0.972949166219, 0.000708037451711, 0.854486426042)\n",
      "neuron 3: (0.220688949162, -0.277828196604, 0.210530590239)\n",
      "final:    (0.0936644128379, 0.825648434725, -0.312860613896, -0.742431468581)\n",
      "\n",
      "300 1.0\n",
      "neuron 1: (0.0323877972029, 0.216736535813, 0.0833023148163)\n",
      "neuron 2: (1.00333546902, 0.0184333807517, 0.879808345042)\n",
      "neuron 3: (0.199522000167, -0.274629632683, 0.248411874856)\n",
      "final:    (0.0872594683507, 0.843380317616, -0.329207737387, -0.832431468581)\n",
      "\n",
      "325 1.0\n",
      "neuron 1: (0.0361829556924, 0.215385905416, 0.0858322002313)\n",
      "neuron 2: (1.04096041871, 0.00595372185199, 0.905051071048)\n",
      "neuron 3: (0.181543470299, -0.281748643337, 0.288463991511)\n",
      "final:    (0.0813001559036, 0.858885589092, -0.344620811646, -0.922431468581)\n",
      "\n",
      "350 1.0\n",
      "neuron 1: (0.0407796750746, 0.214635161629, 0.0890535399367)\n",
      "neuron 2: (1.08913244259, -0.00113971738626, 0.938947006555)\n",
      "neuron 3: (0.16492358223, -0.304357142267, 0.338918250844)\n",
      "final:    (0.0840888145785, 0.873047693737, -0.377040782602, -1.02243146858)\n",
      "\n",
      "375 1.0\n",
      "neuron 1: (0.0447855695835, 0.216971933426, 0.0923917853608)\n",
      "neuron 2: (1.13234384133, 0.0240669318815, 0.974956505509)\n",
      "neuron 3: (0.153689189182, -0.305481598069, 0.361385614812)\n",
      "final:    (0.090069793518, 0.937436066875, -0.383904840083, -1.04243146858)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1.2, 0.7], [-0.3, -0.5], [3.0, 0.1], [-0.1, -1.0], [-1.0, 1.1], [2.1, -3]]\n",
    "labels = [1, -1, 1, -1, -1, 1]\n",
    "a1, a2, a3, a4, b1, b2, b3, b4, c1, c2, c3, c4, d4 = [r.uniform(-0.5, 0.5) for i in range(13)]\n",
    "\n",
    "for l in range(400):\n",
    "    \n",
    "    # forward\n",
    "    i = int(r.random() * len(data))\n",
    "    x, y, label = data[i][0], data[i][1], labels[i]\n",
    "\n",
    "    n1 = max(0, x*a1 + y*b1 + c1)\n",
    "    n2 = max (0, x*a2 + y*b2 + c2)\n",
    "    n3 = max(0, x*a3 + y*b3 + c3)\n",
    "    score = n1*a4 + n2*b4 + n3*c4 + d4\n",
    "\n",
    "    if (l % 25 == 0):\n",
    "        num_cor = 0\n",
    "        for j in range(len(data)):\n",
    "            x_, y_, label_ = data[j][0], data[j][1], labels[j]\n",
    "            n1_, n2_, n3_ = max(0, x_*a1 + y_*b1 + c1), max (0, x_*a2 + y_*b2 + c2), \\\n",
    "                            max(0, x_*a3 + y_*b3 + c3)\n",
    "            score_ = n1_*a4 + n2_*b4 + n3_*c4 + d4\n",
    "            if ((score_ > 0.0 and label_ == 1) or (score_ < 0.0 and label_ == -1)): num_cor += 1\n",
    "            corr = num_cor / float(len(data))\n",
    "        print l, corr\n",
    "        if (corr == 1): print \"neuron 1: (%s, %s, %s)\\n\" % (a1, b1, c1) + \\\n",
    "                              \"neuron 2: (%s, %s, %s)\\n\" % (a2, b2, c2) + \\\n",
    "                              \"neuron 3: (%s, %s, %s)\\n\" % (a3, b3, c3) + \\\n",
    "                              \"final:    (%s, %s, %s, %s)\\n\" % (a4, b4, c4, d4)\n",
    "    \n",
    "    # backward, this will be a loooong chain\n",
    "    pull = 0.0\n",
    "    if (label == 1 and score < 1.0): pull = 1.0\n",
    "    if (label == -1 and score > -1.0): pull = -1.0\n",
    "\n",
    "    # f\n",
    "    da4 = pull * n1\n",
    "    db4 = pull * n2\n",
    "    dc4 = pull * n3\n",
    "    dd4 = pull\n",
    "\n",
    "    dn1 = max(0, pull * a4)\n",
    "    dn2 = max(0, pull * b4)\n",
    "    dn3 = max(0, pull * c4)\n",
    "\n",
    "    # n1\n",
    "    da1 = dn1 * x\n",
    "    db1 = dn1 * y\n",
    "    dc1 = dn1\n",
    "\n",
    "    # n2\n",
    "    da2 = dn2 * x\n",
    "    db2 = dn2 * y\n",
    "    dc2 = dn2\n",
    "\n",
    "    # n3\n",
    "    da3 = dn3 * x\n",
    "    db3 = dn3 * y\n",
    "    dc3 = dn3\n",
    "    \n",
    "    #  no regularization\n",
    "    ss = 0.01\n",
    "    a1 += ss * da1\n",
    "    b1 += ss * db1\n",
    "    c1 += ss * dc1\n",
    "    a2 += ss * da2\n",
    "    b2 += ss * db2\n",
    "    c2 += ss * dc2\n",
    "    a3 += ss * da3\n",
    "    b3 += ss * db3\n",
    "    c3 += ss * dc3\n",
    "    a4 += ss * da4\n",
    "    b4 += ss * db4\n",
    "    c4 += ss * dc4\n",
    "    d4 += ss * dd4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Might be good to interpret in 2 ways. The first is the computational explanation: iteratively, there are 3 neurons which will compose 3 functions for the first batch of weights, then in the end will be fed into another function that weighs the functions before, all tuned via backprop. The classsification task is monitored by checking the examples one by one and seeing if the collection of functions correctly match the labels of the inputs. \n",
    "\n",
    "The second is the visual explanation: the structure enables 3 neurons to create 3 linear boundaries, and in the end will have weights themselves so that they 'battle' for a final non-linear boundary to be created much like this: \n",
    "\n",
    "<img style=\"float: left;\" src=\"https://cdn.pbrd.co/images/H4FxROH.png\"/>\n",
    "\n",
    "In both explanations, it is clear that the hidden layer serves as a way to create more distinct classifiers that separate points into their respective classes. This kind of structure seem to be flexible if there are more dimensions involved and more labels to train for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss/Cost/Objective Function\n",
    "* instead of +/- 'pull', have a function to minimize as an objective, aptly called an objective (or loss or cost) function. i prefer cost.\n",
    "* similar as what was done in one expt above\n",
    "* think of cost as the gap between matching the computer and desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0: [1.2, 0.7] and label: 1\n",
      "score: 0.56 and cost: 0.44\n",
      "example 1: [-0.3, 0.5] and label: -1\n",
      "score: 0.37 and cost: 1.37\n",
      "example 2: [3.0, 2.5] and label: 1\n",
      "score: 1.1 and cost: 0\n",
      "\n",
      "regularization cost for current model is: 0.005\n",
      "total_cost: 1.815\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.815"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1.2, 0.7], [-0.3, 0.5], [3.0, 2.5]]\n",
    "y = [1, -1, 1]\n",
    "w = [0.1, 0.2, 0.3]\n",
    "alpha = 0.1\n",
    "\n",
    "def cost(X,y,w):\n",
    "    total_cost = 0.0\n",
    "    for i in range(len(X)):\n",
    "        score = (X[i][0] * w[0]) + (X[i][1] * w[1]) + w[2]\n",
    "        cost_i = max(0, -y[i] * score + 1)\n",
    "        print \"example %s: %s and label: %s\\nscore: %s and cost: %s\" \\\n",
    "              % (i, X[i], y[i], score, cost_i)\n",
    "        total_cost += cost_i\n",
    "        \n",
    "    reg_cost = alpha * (w[0]**2 + w[1]**2)\n",
    "    total_cost += reg_cost;\n",
    "    print \"\\nregularization cost for current model is: %s\\ntotal_cost: %s\\n\" \\\n",
    "          % (reg_cost, total_cost)\n",
    "    return total_cost\n",
    "\n",
    "cost(X, y, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
