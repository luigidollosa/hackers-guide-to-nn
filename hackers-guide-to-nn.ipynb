{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker's Guide to Neural Networks\n",
    "* Python code w/ personal notes and experiments from [Andrej Karpathy's tutorial](http://karpathy.github.io/neuralnets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Real-valued Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit with Single Gate\n",
    "$f\\left(x,y\\right)\\ =\\ xy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forwardMultiplyGate(x,y):\n",
    "    return x * y\n",
    "\n",
    "forwardMultiplyGate(-2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 1: Random Local Search\n",
    "* throw numbers at the wall and see what sticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = -2, 3\n",
    "best_x, best_y = x, y\n",
    "best_out = -float(\"inf\")\n",
    "tweak_amount = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.9900748582097127, 3.009925141790287, -5.993390245162583)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "for i in range(100):\n",
    "    random_ = (random.random() * 2 - 1)\n",
    "    x_try = x + tweak_amount * random_\n",
    "    y_try = y + tweak_amount * random_\n",
    "    out = forwardMultiplyGate(x_try, y_try)\n",
    "    \n",
    "    if (out > best_out):\n",
    "        best_out, best_x, best_y = out, x_try, y_try\n",
    "\n",
    "best_x, best_y, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 2: Numerical Gradient\n",
    "* finding the derivatives by tweaking the knobs for each pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.00000000000189, -2.0000000000042206)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = -2, 3\n",
    "out = forwardMultiplyGate(x, y)\n",
    "h = 0.0001\n",
    "\n",
    "out_x = forwardMultiplyGate(x + h, y)\n",
    "x_derivative = float((out_x - out) / h)\n",
    "out_y = forwardMultiplyGate(x, y + h)\n",
    "y_derivative = float((out_y - out) / h)\n",
    "\n",
    "x_derivative, y_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{df\\left(x,y\\right)}{dx}=\\frac{f\\left(x+h,\\ y\\right)\\ -\\ f\\left(x,y\\right)}{h}$\n",
    "\n",
    "Think of the derivative as a check and balance. If it is (+), it tells the variable, that if the function ought to increase, this is where the function will be going (combined with the magnitude). The derivative, whether it evaluates to +/- (times magnitude), will be forcing the function to proceed to the direction where the function will increase. It is a value that indicates whether nudge (h) gives a +/- slope when evaluated w/ the original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.87059999999986"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "x += step_size * x_derivative\n",
    "y += step_size * y_derivative\n",
    "out_new = forwardMultiplyGate(x, y)\n",
    "\n",
    "out_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step size is the key here. Turn it (-), the direction will turn the opposite way. When (+), it inclines the derivatives to _increase_ the function. It sort of amplifies, little by little, the force and direction given by the derivatives. It 'commands' the circuit to proceed with the derivatives whatever is their signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to see with more iterations if the gradient is really towards increasing the function.. \n",
    "\n",
    "It is! Except if step to (-)step, opposite direction of gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old x: -0.9999, x derivative: 1.0, new x: -0.99\n",
      "old y: -1.0001, y derivative: -1.0, new y: 0.99\n",
      "out: -0.9801\n",
      "\n",
      "old x: -0.980001, x derivative: 0.99, new x: -0.9801\n",
      "old y: -0.980199, y derivative: -0.99, new y: 0.9801\n",
      "out: -0.96059601\n",
      "\n",
      "old x: -0.960498, x derivative: 0.9801, new x: -0.970299\n",
      "old y: -0.96069402, y derivative: -0.9801, new y: 0.970299\n",
      "out: -0.941480149401\n",
      "\n",
      "old x: -0.941383119501, x derivative: 0.970299, new x: -0.96059601\n",
      "old y: -0.941577179301, y derivative: -0.970298999999, new y: 0.96059601\n",
      "out: -0.922744694428\n",
      "\n",
      "old x: -0.922648634827, x derivative: 0.96059601, new x: -0.9509900499\n",
      "old y: -0.922840754029, y derivative: -0.96059601, new y: 0.9509900499\n",
      "out: -0.904382075009\n",
      "\n",
      "old x: -0.904286976004, x derivative: 0.950990049901, new x: -0.941480149401\n",
      "old y: -0.904477174014, y derivative: -0.950990049899, new y: 0.941480149401\n",
      "out: -0.886384871716\n",
      "\n",
      "old x: -0.886290723701, x derivative: 0.941480149401, new x: -0.932065347907\n",
      "old y: -0.886479019731, y derivative: -0.9414801494, new y: 0.932065347907\n",
      "out: -0.868745812769\n",
      "\n",
      "old x: -0.868652606234, x derivative: 0.932065347907, new x: -0.922744694428\n",
      "old y: -0.868839019304, y derivative: -0.932065347907, new y: 0.922744694428\n",
      "out: -0.851457771095\n",
      "\n",
      "old x: -0.851365496625, x derivative: 0.922744694428, new x: -0.913517247484\n",
      "old y: -0.851550045564, y derivative: -0.922744694427, new y: 0.913517247484\n",
      "out: -0.83451376145\n",
      "\n",
      "old x: -0.834422409725, x derivative: 0.913517247484, new x: -0.904382075009\n",
      "old y: -0.834605113175, y derivative: -0.913517247483, new y: 0.904382075009\n",
      "out: -0.817906937597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "x, y = -1, 1\n",
    "h = 0.0001\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    out = forwardMultiplyGate(x, y)\n",
    "    \n",
    "    out_x = forwardMultiplyGate(x + h, y)\n",
    "    x_derivative = (out_x - out) / h\n",
    "    \n",
    "    out_y = forwardMultiplyGate(x, y + h)\n",
    "    y_derivative = (out_y - out) / h\n",
    "    \n",
    "    x += step_size * x_derivative\n",
    "    y += step_size * y_derivative\n",
    "    out_new = forwardMultiplyGate(x, y)\n",
    "    \n",
    "    print \"old x: %s, x derivative: %s, new x: %s\\nold y: %s, y derivative: %s, new y: %s\\nout: %s\\n\" % \\\n",
    "    (out_x, x_derivative, x, out_y, y_derivative, y, out_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 3: Analytical Gradient\n",
    "* for our function, it turns out that the derivatives of x, y are y, x respectively.\n",
    "\n",
    "$\\frac{df\\left(x,y\\right)}{dx}=\\frac{\\left(x+h\\right)y\\ -\\ xy}{h}=y$\n",
    "\n",
    "Instead of probing with h, compute the derivatives directly for each step because math.\n",
    "Btw, h presumes that whatever the sign of the derivative will be , it corresponds to the rate of growth. ie. increasing a little bit of a variable results to a rate of increase. If that rate is (+), this means growth; if that is (-), opposite of growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.8706"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = -2, 3 # re-initialize\n",
    "x_gradient, y_gradient = y, x # derived from separate evaluation\n",
    "\n",
    "x += step_size * x_gradient\n",
    "y += step_size * y_gradient\n",
    "out_new = forwardMultiplyGate(x, y)\n",
    "\n",
    "out_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuits with Multiple Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forwardMultiplyGate(a, b): return a * b\n",
    "def forwardAddGate(a, b): return a + b\n",
    "\n",
    "def forwardCircuit(x, y, z):\n",
    "    q = forwardAddGate(x, y)\n",
    "    f = forwardMultiplyGate(q, z) \n",
    "    return f\n",
    "\n",
    "x, y, z = -2, 5, -4\n",
    "forwardCircuit(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Backpropagation\n",
    "* the chain rule, is really really useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n",
      "-11.5924\n"
     ]
    }
   ],
   "source": [
    "x, y, z = -2, 5, -4\n",
    "q = forwardAddGate(x, y)\n",
    "f = forwardMultiplyGate(q, z)\n",
    "print f\n",
    "\n",
    "derivative_f_wrt_z = q\n",
    "derivative_f_wrt_q = z\n",
    "\n",
    "derivative_q_wrt_x = 1\n",
    "derivative_q_wrt_y = 1\n",
    "\n",
    "derivative_f_wrt_x = derivative_f_wrt_q * derivative_q_wrt_x\n",
    "derivative_f_wrt_y = derivative_f_wrt_q * derivative_q_wrt_y\n",
    "\n",
    "step_size = 0.01\n",
    "x += step_size * derivative_f_wrt_x\n",
    "y += step_size * derivative_f_wrt_y\n",
    "z += step_size * derivative_f_wrt_z\n",
    "\n",
    "print forwardMultiplyGate(forwardAddGate(x, y), z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to think about this is that the circuit is fitting-a-function problem (in our simple case, the gradients are just increasing the function w/c doesn't optimize anything yet). The chain rule, which is implemented via backpropagation, will signal each layer (and its gradients) how much and where to go in terms of adjustments to satisfy the last function. It is a backward pass, where each layer influences the functions before it to adjust accordingly. The chain rule tells us formally what the sensitivity of the weights (and other variables) to the over-all change of the function. We can sort of trust that every layer and the layer before it communicates locally to produce a desired computation globally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with more iterations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.5924\n",
      "-11.191964\n",
      "-10.798638\n",
      "-10.412368\n",
      "-10.0331\n",
      "-9.66078\n",
      "-9.295354\n",
      "-8.936768\n",
      "-8.584968\n",
      "-8.2399\n",
      "-7.90151\n",
      "-7.569744\n",
      "-7.244548\n",
      "-6.925868\n",
      "-6.61365\n",
      "-6.30784\n",
      "-6.008384\n",
      "-5.715228\n",
      "-5.428318\n",
      "-5.1476\n"
     ]
    }
   ],
   "source": [
    "x, y, z = -2, 5, -4\n",
    "step_size = 0.01\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    dfdz = q\n",
    "    dfdq = z\n",
    "\n",
    "    dqdx = 1\n",
    "    dqdy = 1\n",
    "\n",
    "    dfdx = dfdq * dqdx\n",
    "    dfdy = dfdq * dqdy\n",
    "\n",
    "    x += step_size * dfdx\n",
    "    y += step_size * dfdy\n",
    "    z += step_size * dfdz\n",
    "    \n",
    "    print forwardMultiplyGate(forwardAddGate(x, y), z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, experiment with the chain rule adding a basic cost function at the end.\n",
    "\n",
    "Result: It works! Finds the proper inputs to minimize the function, instead of just ascending the function.   \n",
    "(c gradually decreases, so f gets closer to k). Beautiful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: -1.008\t y: 2.992\t z: 3.994\n",
      "f: 8.0\t c: 2.0\n",
      "\n",
      "x: -1.0157\t y: 2.9843\t z: 3.9882\n",
      "f: 7.9241\t c: 1.8511\n",
      "\n",
      "x: -1.0231\t y: 2.9769\t z: 3.9827\n",
      "f: 7.8513\t c: 1.7137\n",
      "\n",
      "x: -1.0302\t y: 2.9698\t z: 3.9773\n",
      "f: 7.7816\t c: 1.587\n",
      "\n",
      "x: -1.037\t y: 2.963\t z: 3.9722\n",
      "f: 7.7147\t c: 1.4701\n",
      "\n",
      "x: -1.0435\t y: 2.9565\t z: 3.9672\n",
      "f: 7.6506\t c: 1.3622\n",
      "\n",
      "x: -1.0498\t y: 2.9502\t z: 3.9625\n",
      "f: 7.589\t c: 1.2625\n",
      "\n",
      "x: -1.0559\t y: 2.9441\t z: 3.9579\n",
      "f: 7.5299\t c: 1.1703\n",
      "\n",
      "x: -1.0617\t y: 2.9383\t z: 3.9535\n",
      "f: 7.4732\t c: 1.0852\n",
      "\n",
      "x: -1.0673\t y: 2.9327\t z: 3.9492\n",
      "f: 7.4188\t c: 1.0064\n",
      "\n",
      "x: -1.0727\t y: 2.9273\t z: 3.9451\n",
      "f: 7.3665\t c: 0.9336\n",
      "\n",
      "x: -1.0779\t y: 2.9221\t z: 3.9412\n",
      "f: 7.3162\t c: 0.8663\n",
      "\n",
      "x: -1.0829\t y: 2.9171\t z: 3.9373\n",
      "f: 7.268\t c: 0.8039\n",
      "\n",
      "x: -1.0877\t y: 2.9123\t z: 3.9337\n",
      "f: 7.2216\t c: 0.7462\n",
      "\n",
      "x: -1.0924\t y: 2.9076\t z: 3.9302\n",
      "f: 7.1771\t c: 0.6927\n",
      "\n",
      "x: -1.0968\t y: 2.9032\t z: 3.9267\n",
      "f: 7.1342\t c: 0.6432\n",
      "\n",
      "x: -1.1011\t y: 2.8989\t z: 3.9235\n",
      "f: 7.093\t c: 0.5974\n",
      "\n",
      "x: -1.1053\t y: 2.8947\t z: 3.9203\n",
      "f: 7.0534\t c: 0.5549\n",
      "\n",
      "x: -1.1092\t y: 2.8908\t z: 3.9173\n",
      "f: 7.0153\t c: 0.5155\n",
      "\n",
      "x: -1.1131\t y: 2.8869\t z: 3.9143\n",
      "f: 6.9787\t c: 0.4789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y, z, k = -1, 3, 4, 6\n",
    "step_size = 0.001\n",
    "\n",
    "for i in range(20):    \n",
    "   \n",
    "    f = forwardMultiplyGate(forwardAddGate(x, y), z)\n",
    "    c = ((f - k) ** 2) / 2\n",
    "    \n",
    "    dfdz = q\n",
    "    dfdq = z\n",
    "\n",
    "    dqdx = 1\n",
    "    dqdy = 1\n",
    "\n",
    "    dfdx = dfdq * dqdx\n",
    "    dfdy = dfdq * dqdy\n",
    "    \n",
    "    dcdf = k - f # we want to follow the opposite of the gradient, to minimize, not maximize the cost\n",
    "    dcdx = dcdf * dfdx\n",
    "    dcdy = dcdf * dfdy\n",
    "    dcdz = dcdf * dfdz\n",
    "\n",
    "    x += step_size * dcdx\n",
    "    y += step_size * dcdy\n",
    "    z += step_size * dcdz\n",
    "\n",
    "    print \"x: %s\\t y: %s\\t z: %s\\nf: %s\\t c: %s\\n\" % (round(x, 4), round(y, 4), round(z, 4), round(f, 4), round(c, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Neuron\n",
    "* using sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0\n",
      "-6\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "class Unit(object):\n",
    "    def __init__(self, value, grad):\n",
    "        self.value = value\n",
    "        self.grad = grad\n",
    "        \n",
    "class MultiplyGate(object):\n",
    "    def forward(self, u0, u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value * u1.value, 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        self.u0.grad += self.u1.value * self.utop.grad\n",
    "        self.u1.grad += self.u0.value * self.utop.grad\n",
    "        \n",
    "class AddGate(object):\n",
    "    def forward(self, u0, u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value + u1.value, 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        self.u0.grad += 1 * self.utop.grad\n",
    "        self.u1.grad += 1 * self.utop.grad\n",
    "    \n",
    "x = Unit(2,0)\n",
    "y = Unit(-3,0)\n",
    "print x.value, x.grad\n",
    "\n",
    "m = MultiplyGate()\n",
    "print m.forward(x,y).value\n",
    "\n",
    "a = AddGate()\n",
    "print a.forward(x, y).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid function    \n",
    "$sig\\left(x\\right)\\ =\\ \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "derivative   \n",
    "$\\frac{dsig\\left(x\\right)}{dx}=\\ sig\\left(x\\right)\\left(1-sig\\left(x\\right)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952574126822\n",
      "0.880797077978\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "class SigmoidGate(object):\n",
    "    def forward(self, u0):\n",
    "        self.u0 = u0\n",
    "        self.utop = Unit(sigmoid(self.u0.value), 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        s = sigmoid(self.u0.value)\n",
    "        self.u0.grad += (s * (1 - s)) * self.utop.grad\n",
    "\n",
    "print sigmoid(3)\n",
    "sg = SigmoidGate()\n",
    "print sg.forward(x).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Neuron (forward pass)__  \n",
    "* dot product of input & weights, + c (bias term), then feed to sigmoid\n",
    "* it yields a single value\n",
    "\n",
    "__Neuron (backward pass)__   \n",
    "* adjust global vars with computed gradients, the sequence is impt on the chain\n",
    "* started with gradient 1 from the end operation which is the sigmoid\n",
    "* it yields updates to every value (not just a single value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mulg0, self.mulg1 = MultiplyGate(), MultiplyGate()\n",
    "        self.addg0, self.addg1 = AddGate(), AddGate()\n",
    "        self.sg0 = SigmoidGate()\n",
    "\n",
    "    def forward(self, a, b, c, x, y):\n",
    "        ax = self.mulg0.forward(a, x)\n",
    "        by = self.mulg1.forward(b, y)\n",
    "        axby = self.addg0.forward(ax, by)\n",
    "        axbyc = self.addg1.forward(axby, c)\n",
    "        self.s = self.sg0.forward(axbyc)\n",
    "        return self.s\n",
    "    \n",
    "    def backward(self, a, b, c, x, y):\n",
    "        step_size = 0.01\n",
    "        self.s.grad = 1.0\n",
    "        \n",
    "        self.sg0.backward()\n",
    "        self.addg1.backward()\n",
    "        self.addg0.backward()\n",
    "        self.mulg1.backward()\n",
    "        self.mulg0.backward()\n",
    "        \n",
    "        a.value += step_size * a.grad\n",
    "        b.value += step_size * b.grad\n",
    "        c.value += step_size * c.grad\n",
    "        x.value += step_size * x.grad\n",
    "        y.value += step_size * y.grad\n",
    "        \n",
    "        return \"\\nvalues:\\na: %s, x: %s\\nb: %s, y: %s\\nc: %s\\n\" % (a.value, x.value, b.value, y.value, c.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass 1:  0.880797077978\n",
      "\n",
      "backward pass 1:  \n",
      "values:\n",
      "a: 0.998950064146, x: -0.998950064146\n",
      "b: 2.00314980756, y: 3.00209987171\n",
      "c: -2.99895006415\n",
      "\n",
      "forward pass 2:  0.882550181622\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "a, b = Unit(1.0, 0.0), Unit(2.0, 0.0)\n",
    "x, y = Unit(-1.0, 0.0), Unit(3.0, 0.0)\n",
    "c = Unit(-3.0, 0.0)\n",
    "\n",
    "# process / output\n",
    "n = Neuron()\n",
    "print \"forward pass 1: \", n.forward(a, b, c, x, y).value\n",
    "print \"\\nbackward pass 1: \", n.backward(a, b, c, x, y)\n",
    "print \"forward pass 2: \", n.forward(a, b, c, x, y).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Gradients Check\n",
    "* just checking if the gradients are carried out correctly from above.\n",
    "* they're equivalent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.104997583592 0.314944774835 0.104989587345 0.104989587345 0.209971178827\n"
     ]
    }
   ],
   "source": [
    "a, b, c, x, y = 1, 2, -3, -1, 3\n",
    "h = 0.0001\n",
    "\n",
    "def forwardCircuitFast(a, b, c, x, y):\n",
    "    return 1 / (1 + math.exp(-(a*x + b*y + c)))\n",
    "\n",
    "a_grad = (forwardCircuitFast(a+h, b, c, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "b_grad = (forwardCircuitFast(a, b+h, c, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "c_grad = (forwardCircuitFast(a, b, c+h, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "x_grad = (forwardCircuitFast(a, b, c, x+h, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "y_grad = (forwardCircuitFast(a, b, c, x, y+h) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "\n",
    "print a_grad, b_grad, c_grad, x_grad, y_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
